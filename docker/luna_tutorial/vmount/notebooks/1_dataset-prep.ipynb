{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation Tutorial\n",
    "\n",
    "Welcome to the dataset preparation tutorial! In this notebook, we will download the toy data set for the tutorial and prepare the necessary tables used for later analysis. Here are the steps we will review:\n",
    "\n",
    "1. Verify prerequisites\n",
    "2. Create a new project workspace\n",
    "3. Review sample dataset\n",
    "4. Build the proxy table\n",
    "5. Run regional annotation ETL\n",
    "\n",
    "**NOTE**: All of the configuration files for this tutorial have been provided in the container. The host and port values in the configuration files are dynamically set based on your system. \n",
    "\n",
    "**NOTE**: The current working directory is '~/vmount/notebooks'. All file and directory paths specified in the configuration files are relative to the current working directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the software prerequisites for executing tasks with luna packages. These prerequisites have already been baked into this docker container. To view the setup, please see the corresponding dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T14:11:02.591178Z",
     "iopub.status.busy": "2021-11-16T14:11:02.590864Z",
     "iopub.status.idle": "2021-11-16T14:11:05.749394Z",
     "shell.execute_reply": "2021-11-16T14:11:05.748361Z",
     "shell.execute_reply.started": "2021-11-16T14:11:02.591150Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n",
      "LUNA_HOME: /home/pollardw/vmount\n",
      "['/opt/conda/lib/python3.9/site-packages/luna/pathology']\n"
     ]
    }
   ],
   "source": [
    "!python3 --version\n",
    "!echo LUNA_HOME: $LUNA_HOME\n",
    "import luna.pathology\n",
    "print(luna.pathology.__path__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the dask scheduler is running in the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNA_DASK_SCHEDULER: tcp://192.168.176.4:8786\r\n"
     ]
    }
   ],
   "source": [
    "# !ps -ewo pid,ppid,args\n",
    "!echo \"LUNA_DASK_SCHEDULER: $LUNA_DASK_SCHEDULER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a new project workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a luna home space and place the configuration files there. Using a manifest file, we will create a project workspace where your configurations, data, models, and outputs will go for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# project manifest template\n",
      "\n",
      "# MIND project id\n",
      "PROJECT: PRO-12-123\n",
      "\n",
      "# IRB\n",
      "IRB:\n",
      "\n",
      "# project title\n",
      "TITLE: pathology-tutorial\n",
      "\n",
      "# project description\n",
      "DESCRIPTION: End-to-end pathology analysis tutorial\n",
      "\n",
      "DATA_MODALITIES: pathology\n",
      "\n",
      "ROOT_PATH: ../\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p ~/luna\n",
    "cp -R ~/vmount/conf ~/luna\n",
    "cat ~/luna/conf/manifest.yaml\n",
    "cp ~/luna/conf/manifest.yaml \"${LUNA_HOME}/PRO-12-123/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the `manifest.yaml` file in your `vmount/PRO-12-123` directory.  This will be your project workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review sample dataset\n",
    "\n",
    "The data we'll be using for this tutorial is a set of 5 whole slide images (WSI) of ovarian cancer H&E slides, available in the svs file format. Whole slide imaging refers to the scanning of conventional glass slides for research purposes; in this case, these are slides that oncologists have used while inspecting cancer samples.\n",
    "\n",
    "The slides have been downloaded by the script `vmount/provision_girder.py`, which ...\n",
    "  \n",
    "  - Creates an admin user and default assetstore\n",
    "  \n",
    "  - Downloads sample data from [public kitware site](https://data.kitware.com/#user/61b9f3dc4acac99f42ca7678/folder/61b9f4564acac99f42ca7692). to `~/vmount/PRO-12-123/data/toy_data_set/`\n",
    "  \n",
    "  - Creates a collection and adds the slides to your local DSA\n",
    "    \n",
    "If this was successsful, the downloaded svs files will be listed by the `tree` command, below ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/pollardw/vmount/PRO-12-123/data/toy_data_set\u001b[00m\r\n",
      "├── 01OV002-bd8cdc70-3d46-40ae-99c4-90ef77.svs\r\n",
      "├── 01OV002-ed65cf94-8bc6-492b-9149-adc16f.svs\r\n",
      "├── 01OV007-9b90eb78-2f50-4aeb-b010-d642f9.svs\r\n",
      "├── 01OV008-308ad404-7079-4ff8-8232-12ee2e.svs\r\n",
      "├── 01OV008-7579323e-2fae-43a9-b00f-a15c28.svs\r\n",
      "└── \u001b[01;34mtable\u001b[00m\r\n",
      "    ├── \u001b[01;34mANNOTATIONS\u001b[00m\r\n",
      "    │   ├── 01OV002-bd8cdc70-3d46-40ae-99c4-90ef77.annotation.geojson\r\n",
      "    │   ├── 01OV002-ed65cf94-8bc6-492b-9149-adc16f.annotation.geojson\r\n",
      "    │   ├── 01OV007-9b90eb78-2f50-4aeb-b010-d642f9.annotation.geojson\r\n",
      "    │   ├── 01OV008-308ad404-7079-4ff8-8232-12ee2e.annotation.geojson\r\n",
      "    │   ├── 01OV008-7579323e-2fae-43a9-b00f-a15c28.annotation.geojson\r\n",
      "    │   ├── metadata.yml\r\n",
      "    │   └── slide_annotation_dataset_TCGA collection_ov_regional.parquet\r\n",
      "    └── \u001b[01;34mSLIDES\u001b[00m\r\n",
      "        └── slide_ingest_PRO-12-123.parquet\r\n",
      "\r\n",
      "3 directories, 13 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree \"${LUNA_HOME}/PRO-12-123/data/toy_data_set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files may also be viewed through the Minio instance. The URL for this instance is available in the docker-compose terminal log. \n",
    "\n",
    "If you want to import your own data, you can do so from your local filesystem as well as an object store. For more details, refer to the [girder user documentation](https://girder.readthedocs.io/en/latest/user-guide.html#assetstores)\n",
    "\n",
    "To import images from your local filesystem, \n",
    "\n",
    "- Login to DSA with admin/password1\n",
    "- Navigate to **Collections** and use the create collection interface to create a new collection\n",
    "- Under the branch icon on the right, create a folder within the collection\n",
    "- In this folder, use the blue **info** button to access the Unique ID for the folder, and copy and store it somewhere for reference.\n",
    "- Add images to your local computer at `vmount/assetstore` \n",
    "- Navigate to **Admin Console** -> **Assetstores**\n",
    "- From the default assetstore, click on **Import data**\n",
    "- Specify the path to the images you wish to import using absolute path, e.g. `/home/<user>/vmount/assetstore/yourimage`, and specify the destination type as 'Folder' and the destination ID as the ID copied earlier, and click import\n",
    "\n",
    "As the `/assetstore` mount is available to DSA, this import should be much faster than uploading the image through the **Upload files** button in the UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the proxy table\n",
    "\n",
    "Now, we will run the Whole Slide Image (WSI) ETL to build a meta-data catalog of the slides in a proxy table. \n",
    "\n",
    "For reference, ETL stands for extract-transform-load; it is the method that often involves cleaning data, transforming data types, and loading data into different systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41313 instead\n",
      "  warnings.warn(\n",
      "[                                        ] | 0% Completed |  2.8s\u001b[32m2023-08-01 19:00:31.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mget_downscaled_thumbnail ran in 0.72s\u001b[0m\n",
      "\u001b[32m2023-08-01 19:00:31.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mget_downscaled_thumbnail ran in 0.75s\u001b[0m\n",
      "\u001b[32m2023-08-01 19:00:31.894\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mget_downscaled_thumbnail ran in 0.8s\u001b[0m\n",
      "[                                        ] | 0% Completed |  3.4s\u001b[32m2023-08-01 19:00:32.506\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mget_downscaled_thumbnail ran in 1.42s\u001b[0m\n",
      "[########################                ] | 60% Completed |  3.9s\u001b[32m2023-08-01 19:00:33.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mget_downscaled_thumbnail ran in 1.93s\u001b[0m\n",
      "\u001b[32m2023-08-01 19:00:45.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mluna.pathology.cli.slide_etl\u001b[0m:\u001b[36mcli\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1m                                       id  ... properties.aperio.Left\n",
      "0  01OV002-bd8cdc70-3d46-40ae-99c4-90ef77  ...              30.009504\n",
      "1  01OV002-ed65cf94-8bc6-492b-9149-adc16f  ...              30.463894\n",
      "2  01OV007-9b90eb78-2f50-4aeb-b010-d642f9  ...              29.765535\n",
      "3  01OV008-308ad404-7079-4ff8-8232-12ee2e  ...              29.130108\n",
      "4  01OV008-7579323e-2fae-43a9-b00f-a15c28  ...              30.761105\n",
      "\n",
      "[5 rows x 70 columns]\u001b[0m\n",
      "\u001b[32m2023-08-01 19:00:45.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mluna.pathology.cli.slide_etl\u001b[0m:\u001b[36mcli\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWriting to parquet file\u001b[0m\n",
      "\u001b[32m2023-08-01 19:00:45.508\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mluna.common.utils\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m146\u001b[0m - \u001b[34m\u001b[1mcli ran in 45.04s\u001b[0m\n",
      "2023-08-01 19:00:46,896 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48244 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,261 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48666 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,350 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48668 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,378 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48684 remote=tcp://127.0.0.1:40609>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:00:47,424 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48688 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,528 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48824 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,533 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48830 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,601 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48828 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,622 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48838 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,683 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48856 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,684 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48840 remote=tcp://127.0.0.1:40609>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:00:47,725 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48860 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,745 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48858 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48842 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,801 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48854 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,829 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48874 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,839 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48872 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,843 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48878 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48836 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,871 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48886 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,815 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48876 remote=tcp://127.0.0.1:40609>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:00:47,889 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48906 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,903 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48888 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,925 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48884 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,941 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48890 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,954 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48902 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:47,957 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48920 remote=tcp://127.0.0.1:40609>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:00:48,027 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48916 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:48,047 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48904 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:48,099 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48918 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:48,126 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/worker.py\", line 1215, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 400, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/utils_comm.py\", line 385, in retry\n",
      "    return await coro()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 1221, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/core.py\", line 986, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 241, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 144, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:48922 remote=tcp://127.0.0.1:40609>: Stream is closed\n",
      "2023-08-01 19:00:54,058 - distributed.nanny - WARNING - Worker process still alive after 3.199991149902344 seconds, killing\n",
      "2023-08-01 19:00:54,059 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,060 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,061 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,061 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,062 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing\n",
      "2023-08-01 19:00:54,063 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2023-08-01 19:00:54,063 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,064 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,064 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2023-08-01 19:00:54,065 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,066 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,066 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,067 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,067 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,068 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2023-08-01 19:00:54,068 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,069 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2023-08-01 19:00:54,070 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,071 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,072 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,073 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,073 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,074 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,074 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,075 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,076 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,077 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2023-08-01 19:00:54,078 - distributed.nanny - WARNING - Worker process still alive after 3.1999983215332035 seconds, killing\n",
      "2023-08-01 19:00:54,080 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,081 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,082 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,083 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,084 - distributed.nanny - WARNING - Worker process still alive after 3.199998474121094 seconds, killing\n",
      "2023-08-01 19:00:54,085 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,085 - distributed.nanny - WARNING - Worker process still alive after 3.1999989318847657 seconds, killing\n",
      "2023-08-01 19:00:54,086 - distributed.nanny - WARNING - Worker process still alive after 3.1999958801269535 seconds, killing\n",
      "2023-08-01 19:00:54,087 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing\n",
      "2023-08-01 19:00:54,088 - distributed.nanny - WARNING - Worker process still alive after 3.199998474121094 seconds, killing\n",
      "2023-08-01 19:00:54,088 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2023-08-01 19:00:54,089 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,090 - distributed.nanny - WARNING - Worker process still alive after 3.199994201660157 seconds, killing\n",
      "2023-08-01 19:00:54,090 - distributed.nanny - WARNING - Worker process still alive after 3.19999740600586 seconds, killing\n",
      "2023-08-01 19:00:54,091 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing\n",
      "2023-08-01 19:00:54,092 - distributed.nanny - WARNING - Worker process still alive after 3.1999975585937506 seconds, killing\n",
      "2023-08-01 19:00:54,092 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,093 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing\n",
      "2023-08-01 19:00:54,093 - distributed.nanny - WARNING - Worker process still alive after 3.1999978637695317 seconds, killing\n",
      "2023-08-01 19:00:54,094 - distributed.nanny - WARNING - Worker process still alive after 3.1999986267089846 seconds, killing\n",
      "2023-08-01 19:00:54,095 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2023-08-01 19:00:54,095 - distributed.nanny - WARNING - Worker process still alive after 3.1999978637695317 seconds, killing\n",
      "2023-08-01 19:00:54,096 - distributed.nanny - WARNING - Worker process still alive after 3.199995727539063 seconds, killing\n",
      "2023-08-01 19:00:54,096 - distributed.nanny - WARNING - Worker process still alive after 3.1999963378906253 seconds, killing\n",
      "2023-08-01 19:00:54,097 - distributed.nanny - WARNING - Worker process still alive after 3.199995727539063 seconds, killing\n",
      "2023-08-01 19:00:54,097 - distributed.nanny - WARNING - Worker process still alive after 3.1999966430664064 seconds, killing\n",
      "2023-08-01 19:00:54,098 - distributed.nanny - WARNING - Worker process still alive after 3.19999740600586 seconds, killing\n",
      "2023-08-01 19:00:54,099 - distributed.nanny - WARNING - Worker process still alive after 3.1999981689453127 seconds, killing\n",
      "2023-08-01 19:00:54,099 - distributed.nanny - WARNING - Worker process still alive after 3.1999960327148442 seconds, killing\n",
      "2023-08-01 19:00:54,099 - distributed.nanny - WARNING - Worker process still alive after 3.1999983215332035 seconds, killing\n",
      "2023-08-01 19:00:54,100 - distributed.nanny - WARNING - Worker process still alive after 3.1999983215332035 seconds, killing\n",
      "2023-08-01 19:00:54,101 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:00:54,102 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing\r\n",
      "2023-08-01 19:00:54,103 - distributed.nanny - WARNING - Worker process still alive after 3.1999868774414066 seconds, killing\r\n",
      "2023-08-01 19:00:54,103 - distributed.nanny - WARNING - Worker process still alive after 3.1999980163574224 seconds, killing\r\n",
      "2023-08-01 19:00:54,104 - distributed.nanny - WARNING - Worker process still alive after 3.1999981689453127 seconds, killing\r\n",
      "2023-08-01 19:00:54,104 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing\r\n",
      "2023-08-01 19:00:54,105 - distributed.nanny - WARNING - Worker process still alive after 3.1999934387207034 seconds, killing\r\n",
      "2023-08-01 19:00:54,105 - distributed.nanny - WARNING - Worker process still alive after 3.199996795654297 seconds, killing\r\n",
      "2023-08-01 19:00:54,106 - distributed.nanny - WARNING - Worker process still alive after 3.1999971008300783 seconds, killing\r\n",
      "2023-08-01 19:00:54,106 - distributed.nanny - WARNING - Worker process still alive after 3.1999980163574224 seconds, killing\r\n",
      "2023-08-01 19:00:54,107 - distributed.nanny - WARNING - Worker process still alive after 3.1999951171875 seconds, killing\r\n",
      "2023-08-01 19:00:54,107 - distributed.nanny - WARNING - Worker process still alive after 3.199994506835938 seconds, killing\r\n",
      "2023-08-01 19:00:54,108 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing\r\n",
      "2023-08-01 19:00:54,108 - distributed.nanny - WARNING - Worker process still alive after 3.19999740600586 seconds, killing\r\n",
      "2023-08-01 19:00:54,109 - distributed.nanny - WARNING - Worker process still alive after 3.1999975585937506 seconds, killing\r\n",
      "2023-08-01 19:00:54,109 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing\r\n",
      "2023-08-01 19:00:54,110 - distributed.nanny - WARNING - Worker process still alive after 3.1999975585937506 seconds, killing\r\n",
      "2023-08-01 19:00:54,110 - distributed.nanny - WARNING - Worker process still alive after 3.199994201660157 seconds, killing\r\n",
      "2023-08-01 19:00:54,111 - distributed.nanny - WARNING - Worker process still alive after 3.1999969482421875 seconds, killing\r\n",
      "2023-08-01 19:00:54,111 - distributed.nanny - WARNING - Worker process still alive after 3.1999958801269535 seconds, killing\r\n",
      "2023-08-01 19:00:54,112 - distributed.nanny - WARNING - Worker process still alive after 3.1999986267089846 seconds, killing\r\n",
      "2023-08-01 19:00:54,112 - distributed.nanny - WARNING - Worker process still alive after 3.199998474121094 seconds, killing\r\n"
     ]
    }
   ],
   "source": [
    "!slide_etl \"${LUNA_HOME}/PRO-12-123/data/toy_data_set\" \\\n",
    "--project_name PRO-12-123 --comment \"Example Ingestion Job\" \\\n",
    "--no-copy --output-urlpath \"${LUNA_HOME}/PRO-12-123/data/toy_data_set/table/SLIDES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a while. At the end, your proxy table should be generated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** code for the following text has been handled elsewhere, but it is still good to read the following to familiarize yourself with some potential issues with overfitting data.\n",
    "\n",
    "Before we view the table, we must first update it to associate patient IDs with the slides. This is necessary for correctly training and validating the machine learning model in the coming notebooks. Once the slides are divided into \"tiles\" in the next notebook, the tiles are split between the training and validation sets for the ML model. If the tiles do not have patient ID's associated with them, then it is possible for tiles from one individual to appear in both the training and validation of the model; this would cause researchers to have an exaggerated interpretation of the model's accuracy, since we would essentially be validating the model on information that is too near to what it has already seen. \n",
    "\n",
    "Note that we will not be using patient IDs associated with MSK. Instead, we will be using spoof IDs that will suffice for this tutorial. When running this workflow with real data, make sure to include the IDs safely and securely. Run the following block of code to add a 'patient_id' column to the table and store it using Spark (DEPRECATED)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we may view the WSI table! This table should have the metadata associated with the WSI slides that you just collected, including the patient IDs. \n",
    "\n",
    "This table may also be viewed through the Dremio instance. The URL for this instance is available in the docker-compose terminal log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LUNA_HOME = os.environ[\"LUNA_HOME\"]\n",
    "TABLE_DIR = f\"{LUNA_HOME}/PRO-12-123/data/toy_data_set/table\"\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_parquet(f\"{TABLE_DIR}/SLIDES/slide_ingest_PRO-12-123.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the table loads, then you have successfully run the Whole Slide Image (WSI) ETL to database the slides.\n",
    "\n",
    "## Run the regional annotation ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole slide images that you downloaded are images of ovarian cancer, but not every pixel on each slide is a tumor.  In fact, the images typically include tumor cells, normal ovarian cells, lymphoctyes, and more. *Note: A non-expert annotated this slide for demo purposes only.*\n",
    "\n",
    "The regional annotation ETL performs the following steps\n",
    "\n",
    "- Download DSA json annotations\n",
    "- Convert DSA jsons to GeoJSON format, which is compatible with downstream applications\n",
    "- Save configs in your `~/vmount/PRO-12-123/configs/REGIONAL_METADATA_RESULTS`\n",
    "- Save parquet table in your `~/vmount/PRO-12-123/tables/REGIONAL_METADATA_RESULTS `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the regional annotation ETL, we use the `dsa_annotation` CLI. For more details on the `dsa_annotation` tool, and the annotations we support, please checkout the `7_dsa-annotation.ipynb` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"{TABLE_DIR}/ANNOTATIONS\"\n",
    "!dsa_annotation http://girder:8080/api/v1 \\\n",
    "    --collection-name \"TCGA collection\" \\\n",
    "    --annotation-name \"ov_regional\" \\\n",
    "    --username admin --password password1 \\\n",
    "    --output-urlpath \"{TABLE_DIR}/ANNOTATIONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the regional annotation ETL was correctly run, you can examine the regional annotations table.  This table contains the metadata saved by the ETL.  It includes paths to the bitmap files, numpy files, and geoJSON files that were mentioned before.  To load the table, run the following cell,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyarrow.parquet import read_table\n",
    "annotations_table = f\"{TABLE_DIR}/ANNOTATIONS/slide_annotation_dataset_TCGA collection_ov_regional.parquet\"\n",
    "pd.read_parquet(annotations_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets get our geojsons and join on slide ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(annotations_table) \\\n",
    "    .query(\"type=='geojson'\")[['slide_geojson']] \\\n",
    "    .join(\n",
    "        pd.read_parquet(f\"{TABLE_DIR}/SLIDES/slide_ingest_PRO-12-123.parquet\")['id']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have successfully set up your workspace, downloaded the data, and run both the pathology and regional annotation ETLs to prepare your data. You are ready to move on to the tiling notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
